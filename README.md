# ğŸ§  **AI Co-Workspace**

**Claude-style AI co-workspace built for engineers.**
Persistent memory, artifact intelligence, transparent RAG, and multi-model LLM orchestration â€” designed for real project workflows, not just chat.

---

## âœ¨ **Overview**

AI Co-Workspace is an **open-source, workspace-centric AI platform** where conversations evolve into artifacts, artifacts become memory, and memory actively informs future reasoning.

Unlike traditional AI chat tools, this system treats **context as infrastructure** â€” observable, editable, and extensible.

Think **Claude Workspaces**, but:

* Open-source
* Multi-model
* Local-first
* RAG-transparent
* Engineer-controlled

---

## ğŸ¯ **Why This Project Exists**

Modern LLM tools are powerful, but they:

* Forget long-running projects
* Hide how memory and context are selected
* Lock users into a single model
* Treat artifacts as disposable outputs
* Offer no observability or control

**AI Co-Workspace explores what an AI system looks like when memory, artifacts, and context are first-class, inspectable systems.**

---

## ğŸš€ **Core Capabilities**

### ğŸ—‚ï¸ Workspace-First Design

Each workspace is fully isolated and owns:

* Chats
* Artifacts
* Files
* Memory
* Context rules

No cross-project contamination. No global memory leaks.

---

### ğŸ’¬ **Intelligent Chat System**

* Workspace-scoped chat sessions
* Message history with summarization
* Streaming-ready API design
* Built for long-running collaboration

---

### ğŸ§  **Persistent Memory (RAG-Powered)**

Memory is not magic â€” itâ€™s **explicit and inspectable**.

Memory sources:

* Chat summaries
* Artifacts (code, docs, configs)
* Uploaded files

Retrieval:

* Keyword + semantic search
* Vector DB backed (Chroma / Qdrant)
* Workspace-scoped context injection

---

### ğŸ“„ **Artifact Intelligence**

Artifacts are not just displayed â€” they are **understood**.

* Automatic artifact detection
* Artifact lifecycle: ```Detect â†’ Create â†’ Update â†’ Persist```
* Artifacts feed memory and future reasoning
* Workspace-aware artifact management

---

### ğŸ”Œ **Multi-Model LLM Support**

Model-agnostic by design.

Supported:

* Ollama (local LLMs)
* OpenAI
* Extensible model registry

Switch models per workspace or task â€” no lock-in.

---

### ğŸ“Š **Observability (Planned)**

Designed for engineers who care about internals:

* Context inspection
* Retrieved memory visibility
* Token usage tracking
* Cost awareness

---

## ğŸ†š **What This Has That Claude Workspaces Donâ€™t**

| Capability             | Claude | AI Co-Workspace |
| ---------------------- | ------ | --------------- |
| Editable memory        | âŒ     | âœ…             |
| Transparent RAG        | âŒ     | âœ…             |
| Artifact lifecycle     | âŒ     | âœ…             |
| Multi-model support    | âŒ     | âœ…             |
| Local LLMs             | âŒ     | âœ…             |
| Workspace export       | âŒ     | âœ…             |
| Extensible APIs        | âŒ     | âœ…             |

> Claude is a closed product.
> AI Co-Workspace is an **open platform**.

---

## ğŸ—ï¸ **Architecture Flow**

```
User Prompt
   â†“
Workspace Context Builder
   â†“
Memory Retrieval (RAG)
   â†“
Prompt Builder
   â†“
LLM (Ollama / OpenAI)
   â†“
Response
   â†“
Artifact Detection
   â†“
Memory Update
```

---

## ğŸ“ **Repository Structure**

```
ai-co-workspace/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ mvp-execution-flow.md
â”‚   â”œâ”€â”€ memory-system.md
â”‚   â”œâ”€â”€ rag-design.md              # ğŸ†• RAG explained clearly
â”‚   â”œâ”€â”€ artifact-lifecycle.md
â”‚   â”œâ”€â”€ api-contracts.md
â”‚   â””â”€â”€ screenshots/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â”œâ”€â”€ logging.py
â”‚   â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â”‚   â””â”€â”€ lifespan.py
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”œâ”€â”€ session.py
â”‚   â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚   â”‚       â”œâ”€â”€ workspace.py
â”‚   â”‚   â”‚       â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚       â”œâ”€â”€ message.py
â”‚   â”‚   â”‚       â”œâ”€â”€ artifact.py
â”‚   â”‚   â”‚       â””â”€â”€ memory.py
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚   â”œâ”€â”€ workspace.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ message.py
â”‚   â”‚   â”‚   â”œâ”€â”€ artifact.py
â”‚   â”‚   â”‚   â””â”€â”€ memory.py
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ deps.py
â”‚   â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚   â”‚       â”œâ”€â”€ workspaces.py
â”‚   â”‚   â”‚       â”œâ”€â”€ chats.py
â”‚   â”‚   â”‚       â”œâ”€â”€ messages.py
â”‚   â”‚   â”‚       â”œâ”€â”€ artifacts.py
â”‚   â”‚   â”‚       â”œâ”€â”€ files.py
â”‚   â”‚   â”‚       â”œâ”€â”€ rag.py            # ğŸ†• RAG query endpoint
â”‚   â”‚   â”‚       â””â”€â”€ health.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ llm/                  # PHASE 2
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ollama.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ openai.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ model_registry.py
â”‚   â”‚   â”‚   â”œâ”€â”€ prompt/               # PHASE 3
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ builder.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ context.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ system_rules.py
â”‚   â”‚   â”‚   â”œâ”€â”€ memory/               # MEMORY LAYER
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ summarizer.py      # chat â†’ summary
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py       # lightweight recall
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ semantic_retriever.py  # ğŸ†• RAG core
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ memory_manager.py
â”‚   â”‚   â”‚   â”œâ”€â”€ artifacts/            # PHASE 4
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ detector.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ creator.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ updater.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ artifact_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ vector/               # ğŸ†• RAG INFRA
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py     # text â†’ vector
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chroma_store.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ qdrant_store.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ vector_router.py
â”‚   â”‚   â”‚   â””â”€â”€ workspaces/
â”‚   â”‚   â”‚       â”œâ”€â”€ context_builder.py
â”‚   â”‚   â”‚       â””â”€â”€ permissions.py
â”‚   â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”‚   â”œâ”€â”€ summarization.py
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding.py           # ğŸ†• background embedding
â”‚   â”‚   â”‚   â””â”€â”€ memory_cleanup.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ token_counter.py
â”‚   â”‚       â”œâ”€â”€ text.py
â”‚   â”‚       â””â”€â”€ time.py
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ test_workspaces.py
â”‚   â”‚   â”œâ”€â”€ test_chats.py
â”‚   â”‚   â”œâ”€â”€ test_artifacts.py
â”‚   â”‚   â”œâ”€â”€ test_memory.py
â”‚   â”‚   â””â”€â”€ test_rag.py                # ğŸ†• RAG tests
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ (unchanged â€“ RAG is backend-transparent)
â”œâ”€â”€ vectorstore/
â”‚   â”œâ”€â”€ chroma/
â”‚   â””â”€â”€ qdrant/
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ (docker / k8s / terraform)
â””â”€â”€ scripts/
    â”œâ”€â”€ seed_workspace.py
    â”œâ”€â”€ migrate_db.py
    â””â”€â”€ dev.sh
```

---

## ğŸ› ï¸ **Tech Stack**

### **Backend**

* FastAPI
* SQLAlchemy
* Async service architecture
* Modular domain design

### **LLMs**

* Ollama (local-first)
* OpenAI
* Model registry abstraction

### **Memory & RAG**

* Chroma / Qdrant
* Embedding abstraction
* Semantic + keyword retrieval

### **Frontend**

* Next.js
* TailwindCSS
* Workspace-centric UI

### **Infrastructure**

* Docker & Docker Compose
* Kubernetes manifests
* Terraform modules

---

## ğŸ§ª **What This Project Is NOT**

* âŒ Not an LLM training platform
* âŒ Not a fine-tuning pipeline
* âŒ Not a Claude UI clone
* âŒ Not tied to one vendor or cloud

This project focuses on **LLM orchestration, memory systems, and workspace intelligence**.

---

## ğŸ—ºï¸ **MVP Execution Phases**

1ï¸âƒ£ Backend workspace & chat APIs
2ï¸âƒ£ Ollama LLM integration
3ï¸âƒ£ Prompt builder + memory retrieval
4ï¸âƒ£ Artifact detection & lifecycle
5ï¸âƒ£ Frontend workspace UI
6ï¸âƒ£ Vector DB & semantic memory
7ï¸âƒ£ Polishing, observability & docs

---

## ğŸ“¦ **Data Ownership & Portability**

* Workspace export (JSON / Markdown)
* Local-first execution
* No vendor lock-in
* Your data, your models, your control

---

## ğŸ¤ **Who This Is For**

* Cloud & DevOps engineers
* AI platform builders
* RAG system designers
* Open-source contributors
* Teams building serious, long-running AI projects

---

## ğŸ“œ **License**

MIT License â€” build freely, fork openly, ship boldly.

---

## ğŸ§­ **Roadmap Highlights**

* Memory pinning & importance scoring
* Artifact versioning & diffs
* Context inspection UI
* Plugin & webhook system
* CI/CD & GitHub integrations

---

## â­ **Final Note**

This project isnâ€™t about replacing Claude.

It answers a different question:

> **â€œWhat does an AI workspace look like when engineers control memory, context, and models?â€**

---
